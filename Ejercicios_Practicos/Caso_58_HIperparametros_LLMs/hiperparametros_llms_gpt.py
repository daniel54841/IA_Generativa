# -*- coding: utf-8 -*-
"""Hiperparametros_LLMs_GPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DXgYW3dZic8hEKX4h7X3uNFw2YDpJv2f

# Hiperparámetros de los LLMs (GPT)

<div style="background-color:#D9EEFF;color:black;padding:2%;">
<h2>Enunciado del caso práctico</h2>

En este caso práctico, se propone al alumno la realización de pruebas modificando los hiperparámetros del LLM GPT-3.5-Turbo ofrecido por OpenAI: https://platform.openai.com/docs/api-reference/chat/create
</div>

# Resolución del caso práctico

## 0. Instalación de librerías externas
"""

!pip install openai==0.28

"""## 1. Lectura de la API Key"""

import openai

with open("/content/drive/MyDrive/api-keys/secret-key.txt") as f:
  openai.api_key = f.readline()

"""## 2. Selección del modelo

OpenAI nos ofrece una amplia variedad de modelos que podemos utilizar de manera programática para diferentes tareas.

Nosotros estamos interesados en LLMs específicamente entrenados para entender lenguaje natural e interactuar con el usuario en forma de chat.

Teniendo esto en cuenta, podemos elegir entre los siguientes: https://platform.openai.com/docs/guides/gpt

Para este caso práctico vamos a seleccionar `gpt-3.5-turbo`
"""

def obtener_completion(prompt, temperatura=1, top_p=1, frequency_penalty=0.0, model="gpt-3.5-turbo"):
  mensaje = [{"role": "user", "content": prompt}]
  respuesta = openai.ChatCompletion.create(
      model=model,
      messages=mensaje,
      temperature=temperatura,
      top_p=top_p,
      frequency_penalty=frequency_penalty
  )
  return respuesta.choices[0].message["content"]

"""## 3. Temperatura

La `temperatura` controla la aleatoriedad de las respuestas generadas. Una temperatura más baja (por ejemplo, 0.2) hace que las respuestas sean más coherentes y predecibles, mientras que una temperatura más alta (por ejemplo, 0.8) hace que las respuestas sean más creativas y diversas, pero también pueden ser menos precisas.
"""

review="""
Sinceramente me ha cambiado la vida, vivo en una casa con diferentes plantas y compré dos altavoces inteligentes, uno para el salón y otro para mi habitación y \
para comunicarnos mis padres y yo de una planta a otra y no tener que estar gritando es lo mejor, es muy rapida he inteligente. La compre \
junto con unos enchufes y funciona de maravilla es genial. El altavoz para escuchar musica es muy potente y si tienes dos ( como es mi caso) \
se pueden sincronizar y escuchar musica por todas partes! Se conecta rapido al móvil, a la televisión y contesta a prácticamente todo.
Con lo unico que he tenido fallo ha sido con alguna alarma que he programado y no ha sonado, por el resto todo genial, ahora cuando programo \
una alarma o pongo dos o compruebo con la app.
"""
sentimiento="positivo"

prompt = f"""
Eres una IA asistente de atención al cliente.
Tu tarea es enviar una respuesta por correo electrónico a un valioso cliente.
Dado el correo electrónico del cliente delimitado por ```, \
Genera una respuesta para agradecer al cliente por su reseña.
Si el sentimiento es positivo o neutral, agradéceles por \
su reseña.
Si el sentimiento es negativo, discúlpate y sugiere que \
pueden comunicarse con el servicio al cliente.
Asegúrate de usar detalles específicos de la reseña.
Escribe en un tono conciso y profesional.
Firma el correo electrónico como `Agente de atención al cliente de IA`.
Reseña del cliente: ```{review}```
Sentimiento de la reseña: {sentimiento}
"""

respuesta = obtener_completion(prompt, temperatura=1)

print(respuesta)

"""## 4. Top_p

El valor de `top_p` se establece en un rango entre 0 y 1, donde representa una fracción acumulativa de probabilidad. Si estableces `top_p` en un valor, el modelo calculará las palabras más probables en función de ese valor y generará texto en función de esas palabras. Por ejemplo, si estableces `top_p` en 0.8, el modelo seleccionará las palabras que representan el 80% de la probabilidad acumulativa en lugar de considerar todas las palabras posibles.
"""

respuesta = obtener_completion(prompt, temperatura=0, top_p=0.2)

print(respuesta)

"""## 5. Frequency penalty

Número entre -2.0 y 2.0. Los valores positivos penalizan los nuevos tokens en función de su frecuencia existente en el texto hasta ese momento, disminuyendo la probabilidad del modelo de repetir la misma línea de manera idéntica.
"""

respuesta = obtener_completion(prompt, frequency_penalty=1.0)

print(respuesta)